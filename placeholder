
"""

## text object
contains following attributes:
    Text(
        speaker: speaker_name,
        start to end list: [5, 6, 7 .. 15], # formatted from start to end time
        transcription: utterance,
        index: element.index
    )

## append Text objects into audio_texts
audio_texts = [Text]

## parsed_data: empty dictionary to hold each speaker sequence
parsed_data = {}

## loop through audio_texts to generate parsed_data
for txt_obj in audio_texts:
    ## add speaker to parsed_data if not exist or add value if exists
    if txt_obj.speaker in parsed_data:
        parsed_data[txt_obj.speaker].append(
            (text_obj.index, text_obj.start to end list, text_obj.text)
        )
    else:
        parsed_data[txt_obj.speaker] = [
            (text_obj.index, text_obj.start to end list, text_obj.text)
        ]

flattened_dictionary = {}

## extract seconds where speaker has spoken
for speaker in parsed_data:
    phrase_map = [speaker_tuple[1] for speaker_tuple in speaker]
    flattened_list = list(itertools.chain.from_iterable(phrase_map))
    result = []
    for i in range(length of video in seconds+1, step=1):
        if i in flattened_list:
            result.append("speech")
        else:
            result.append("silence")
    flattened_dictionary[speaker] = result

## flattened_dictionary can now replace data in line 33 animator.py

## parsed_data_dictionary sample content
            av_1: [
                (1, [2, 3, 4], "killer man"),
                (2, [6, 7, 8], "killer man"),
                (3, [2, 3, 4], "killer man"),
                ]

            av_2: [
                ()
            ]


## flattened_dictionary sample content
    av_1: ["speech", "speech", "silence"]
    av_2: ["silence", "speech", "silence"]
"""
